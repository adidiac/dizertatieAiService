{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51722246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: psutil in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: setuptools in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.3.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers accelerate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7703d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pandas as pd, numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "import evaluate\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3493e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBTI files at: /Users/adrian/.cache/kagglehub/datasets/datasnaek/mbti-type/versions/1\n",
      "(8675, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will download & unzip into a folder, e.g. \"./datasnaek-mbti-type\"\n",
    "path = kagglehub.dataset_download(\"datasnaek/mbti-type\")\n",
    "print(\"MBTI files at:\", path)\n",
    "\n",
    "# Load the CSV\n",
    "mbti_df = pd.read_csv(os.path.join(path, \"mbti_1.csv\"))\n",
    "print(mbti_df.shape)    # ~8600 rows × 2 cols\n",
    "mbti_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364c0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    # split posts, remove URLs/mentions, collapse whitespace\n",
    "    txt = \" \".join(s.split(\"|||\"))\n",
    "    txt = re.sub(r\"http\\S+|www\\.\\S+\", \"\", txt)\n",
    "    txt = re.sub(r\"[@#]\\S+\", \"\", txt)\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "\n",
    "mbti_df[\"text\"] = mbti_df[\"posts\"].map(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0c5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_to_psycho(mbti: str):\n",
    "    I, N, T, J = [(c==flag) for (c,flag) in zip(mbti, [\"I\",\"N\",\"T\",\"J\"])]\n",
    "    E, S, F, P = [not x for x in (I, N, T, J)]\n",
    "    # awareness: N⇒0.8, else 0.2\n",
    "    awareness = 0.8 if N else 0.2\n",
    "    # conscientiousness: J⇒0.8, else 0.2\n",
    "    conscientiousness = 0.8 if J else 0.2\n",
    "    # neuroticism: base 0.3 +0.3 if F +0.2 if I  ⇒ range [0.3,0.8]\n",
    "    neuro = 0.3 + (0.3 if F else 0) + (0.2 if I else 0)\n",
    "    # stress tolerance = 1 – neuroticism\n",
    "    stress = 1 - neuro\n",
    "    # risk tolerance:\n",
    "    if E and T:\n",
    "        risk = 0.8\n",
    "    elif E and F:\n",
    "        risk = 0.4\n",
    "    elif I and T:\n",
    "        risk = 0.6\n",
    "    else:  # I & F\n",
    "        risk = 0.2\n",
    "    return awareness, conscientiousness, stress, neuro, risk\n",
    "\n",
    "# apply mapping\n",
    "mapped = mbti_df[\"type\"].map(lambda t: mbti_to_psycho(t))\n",
    "mbti_df[[\"awareness\",\"conscientiousness\",\"stress\",\"neuroticism\",\"risk_tolerance\"]] = \\\n",
    "    pd.DataFrame(mapped.tolist(), index=mbti_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bd7aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: mbti_psychometrics.csv\n"
     ]
    }
   ],
   "source": [
    "out_csv = \"mbti_psychometrics.csv\"  # ⬅️ adjust filename if you like\n",
    "mbti_df[[\"text\",\"awareness\",\"conscientiousness\",\"stress\",\"neuroticism\",\"risk_tolerance\"]] \\\n",
    "    .to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a35e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(\n",
    "    mbti_df[[\"text\",\"awareness\",\"conscientiousness\",\"stress\",\"neuroticism\",\"risk_tolerance\"]]\n",
    ")\n",
    "split = ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds, eval_ds = split[\"train\"], split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8d9427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6940/6940 [00:08<00:00, 863.25 examples/s]\n",
      "Map: 100%|██████████| 1735/1735 [00:02<00:00, 796.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer  = BertTokenizerFast.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(x):\n",
    "    return tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds  = eval_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds.set_format(type=\"torch\",\n",
    "                    columns=[\"input_ids\",\"attention_mask\",\n",
    "                             \"awareness\",\"conscientiousness\",\"stress\",\"neuroticism\",\"risk_tolerance\"])\n",
    "eval_ds.set_format(type=\"torch\",\n",
    "                   columns=[\"input_ids\",\"attention_mask\",\n",
    "                            \"awareness\",\"conscientiousness\",\"stress\",\"neuroticism\",\"risk_tolerance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2fbb09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6940/6940 [00:01<00:00, 3511.94 examples/s]\n",
      "Map: 100%|██████████| 1735/1735 [00:00<00:00, 3824.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Rebuild train_ds so it has a single 'labels' list field:\n",
    "import torch\n",
    "\n",
    "def pack_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        example[\"awareness\"],\n",
    "        example[\"conscientiousness\"],\n",
    "        example[\"stress\"],\n",
    "        example[\"neuroticism\"],\n",
    "        example[\"risk_tolerance\"],\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "train_ds = train_ds.map(pack_labels)\n",
    "eval_ds  = eval_ds.map(pack_labels)\n",
    "\n",
    "# Now reset formats:\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
    "eval_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ba3675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/hnm3c8wn6518hm1_77tn6jgm0000gn/T/ipykernel_45624/377952455.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2604' max='2604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2604/2604 1:20:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.036300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/adrian/Desktop/Master/Dizertatie/proiect/riskManagement/riskManagementBackend/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='217' max='217' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [217/217 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Eval MSE: 0.04042550548911095\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# 1. Define a metrics function\n",
    "mse_metric = evaluate.load(\"mse\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred  # both are np arrays of shape (batch_size, 5)\n",
    "    return {\"mse\": float(np.mean((preds - labels) ** 2))}\n",
    "\n",
    "# 2. Set up TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mbti-psych-reg\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    # no `evaluation_strategy`, just evaluate at the end\n",
    ")\n",
    "\n",
    "# 3. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,          # ensures padding & everything if needed\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 4. Train\n",
    "trainer.train()\n",
    "\n",
    "# 5. Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final Eval MSE:\", metrics[\"eval_mse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da84bfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('psychometric_model/tokenizer_config.json',\n",
       " 'psychometric_model/special_tokens_map.json',\n",
       " 'psychometric_model/vocab.txt',\n",
       " 'psychometric_model/added_tokens.json',\n",
       " 'psychometric_model/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"psychometric_model\")\n",
    "tokenizer.save_pretrained(\"psychometric_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f8b595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I always double-check every link before clicking on it.\n",
      "  Awareness:         0.67\n",
      "  Conscientiousness: 0.46\n",
      "  Stress:            0.35\n",
      "  Neuroticism:       0.67\n",
      "  Risk tolerance:    0.40\n",
      "\n",
      "Text: I often ignore software updates if I'm busy.\n",
      "  Awareness:         0.54\n",
      "  Conscientiousness: 0.53\n",
      "  Stress:            0.42\n",
      "  Neuroticism:       0.61\n",
      "  Risk tolerance:    0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Test the trained model on new texts\n",
    "\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 11.1 Load the fine-tuned model and tokenizer\n",
    "checkpoint = \"psychometric_model\"  # wherever you saved it\n",
    "tokenizer  = BertTokenizerFast.from_pretrained(checkpoint)\n",
    "model      = BertForSequenceClassification.from_pretrained(checkpoint, problem_type=\"regression\")\n",
    "model.eval()\n",
    "\n",
    "# 11.2 Define some new sentences to score\n",
    "test_texts = [\n",
    "    \"I always double-check every link before clicking on it.\",\n",
    "    \"I often ignore software updates if I'm busy.\"\n",
    "]\n",
    "\n",
    "# 11.3 Tokenize\n",
    "inputs = tokenizer(\n",
    "    test_texts,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 11.4 Run the model (no grad)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "preds = outputs.logits  # shape: (batch_size, 5)\n",
    "\n",
    "# 11.5 Print out each of the five psychometric scores\n",
    "for text, scores in zip(test_texts, preds):\n",
    "    awareness, conscientiousness, stress, neuroticism, risk_tolerance = scores.tolist()\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Awareness:         {awareness:.2f}\")\n",
    "    print(f\"  Conscientiousness: {conscientiousness:.2f}\")\n",
    "    print(f\"  Stress:            {stress:.2f}\")\n",
    "    print(f\"  Neuroticism:       {neuroticism:.2f}\")\n",
    "    print(f\"  Risk tolerance:    {risk_tolerance:.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
